{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeAHpJJIYQSd/OPuXJem46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjang6251/intelligentsoftware/blob/main/12%EC%A3%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_F-duCk1PYpl"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",  # 최신 Embedding 모델\n",
        "        input=texts\n",
        "    )\n",
        "    return [item.embedding for item in response.data]\n",
        "\n",
        "# List of color words to embed\n",
        "color_words = [\"red\", \"blue\", \"yellow\", \"green\", \"violet\", \"cyan\", \"black\", \"white\"]\n",
        "\n",
        "# Get embeddings for the color words\n",
        "color_embeddings = get_embeddings(color_words)\n",
        "\n",
        "# Print the embeddings for each color word\n",
        "for word, embedding in zip(color_words, color_embeddings):\n",
        "    print(f\"{word}: {embedding[:5]}...\")  # Print only the first 5 numbers\n",
        "    print(len(embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewL4x50DRxTD",
        "outputId": "0afb696b-f583-409f-d3ac-52b8e1f63b60"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red: [-0.02211996167898178, -0.010933708399534225, -0.0028299882542341948, 0.019148845225572586, 0.01706906408071518]...\n",
            "1536\n",
            "blue: [-0.0011275681899860501, -0.016529185697436333, -0.013120132498443127, 0.014094147831201553, 0.005262590479105711]...\n",
            "1536\n",
            "yellow: [-0.014219511300325394, -0.017935020849108696, 0.027377447113394737, 0.057157404720783234, -0.010168765671551228]...\n",
            "1536\n",
            "green: [0.006280624307692051, -0.0011062989942729473, 0.0616002157330513, 0.015412422828376293, 0.002596053294837475]...\n",
            "1536\n",
            "violet: [0.007715038955211639, -0.023039432242512703, -0.015892451629042625, -0.021731574088335037, 5.815796976094134e-05]...\n",
            "1536\n",
            "cyan: [0.042823027819395065, -0.002782088005915284, -0.03496422991156578, 0.053410161286592484, 0.02467365562915802]...\n",
            "1536\n",
            "black: [0.011150840669870377, -0.012059953063726425, -0.01933285780251026, -0.013608287088572979, 0.005955400876700878]...\n",
            "1536\n",
            "white: [0.0032487320713698864, -0.02826567552983761, -0.003470624564215541, 0.03234697878360748, 0.003618552815169096]...\n",
            "1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def edit_text(input_text, instruction):\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        input=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant that edits text.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Please edit the following text: '{input_text}'. Instruction: {instruction}\"\n",
        "            }\n",
        "        ],\n",
        "        max_output_tokens=300\n",
        "    )\n",
        "    edited = response.output_text\n",
        "    return edited\n",
        "\n",
        "input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "instruction = \"Change 'fox' to 'cat' and change the tense to past.\"\n",
        "\n",
        "edited_text = edit_text(input_text, instruction)\n",
        "print(edited_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e23tmdjIj_bt",
        "outputId": "9f96417d-eb2e-4fd3-927b-fed5bdb55f47"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown cat jumped over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Moderation\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Function to moderate text using the Moderation API\n",
        "def moderate_text(input_text):\n",
        "    response = client.moderations.create(\n",
        "        model=\"omni-moderation-latest\",\n",
        "        input=input_text\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Example input texts\n",
        "input_texts = [\n",
        "    \"I want to harm myself.\",\n",
        "    \"You are an amazing person!\",\n",
        "    \"Let's meet at 8 PM.\"\n",
        "]\n",
        "\n",
        "for text in input_texts:\n",
        "    moderation_result = moderate_text(text)\n",
        "    print(f\"Input: {text}\")\n",
        "    print(\"Moderation Result:\", moderation_result)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ckdqgTQdjrNT",
        "outputId": "bda45354-de25-48d5-ee58-05d9f160cb7d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I want to harm myself.\n",
            "Moderation Result: ModerationCreateResponse(id='modr-9386', model='omni-moderation-latest', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=True, self_harm_instructions=False, self_harm_intent=True, sexual=False, sexual_minors=False, violence=True, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=True, self-harm/instructions=False, self-harm=True, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.0005689574642911562, harassment_threatening=0.0007696328798416084, hate=1.7952796934677738e-05, hate_threatening=8.888084683809127e-06, illicit=0.0052831760048374175, illicit_violent=2.4156629828672456e-05, self_harm=0.9746147566202028, self_harm_instructions=0.00028151729221049604, self_harm_intent=0.9885491614739444, sexual=8.559006367452268e-05, sexual_minors=6.205049602300744e-06, violence=0.4023404152908457, violence_graphic=0.001610160050623429, harassment/threatening=0.0007696328798416084, hate/threatening=8.888084683809127e-06, illicit/violent=2.4156629828672456e-05, self-harm/intent=0.9885491614739444, self-harm/instructions=0.00028151729221049604, self-harm=0.9746147566202028, sexual/minors=6.205049602300744e-06, violence/graphic=0.001610160050623429), flagged=True)])\n",
            "----------------------------------------\n",
            "Input: You are an amazing person!\n",
            "Moderation Result: ModerationCreateResponse(id='modr-6040', model='omni-moderation-latest', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=0.0006602641499937108, harassment_threatening=1.0391067562761452e-05, hate=6.10885510164857e-06, hate_threatening=1.5057017254045334e-07, illicit=6.40200641038395e-06, illicit_violent=4.198630823683514e-06, self_harm=1.1235328063870752e-05, self_harm_instructions=1.1300808333434096e-06, self_harm_intent=5.144221374220898e-06, sexual=1.15919343186331e-05, sexual_minors=1.3007128466476034e-06, violence=0.0005323933551511255, violence_graphic=4.832563818725537e-06, harassment/threatening=1.0391067562761452e-05, hate/threatening=1.5057017254045334e-07, illicit/violent=4.198630823683514e-06, self-harm/intent=5.144221374220898e-06, self-harm/instructions=1.1300808333434096e-06, self-harm=1.1235328063870752e-05, sexual/minors=1.3007128466476034e-06, violence/graphic=4.832563818725537e-06), flagged=False)])\n",
            "----------------------------------------\n",
            "Input: Let's meet at 8 PM.\n",
            "Moderation Result: ModerationCreateResponse(id='modr-4692', model='omni-moderation-latest', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, harassment/threatening=False, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False), category_applied_input_types=CategoryAppliedInputTypes(harassment=['text'], harassment_threatening=['text'], hate=['text'], hate_threatening=['text'], illicit=['text'], illicit_violent=['text'], self_harm=['text'], self_harm_instructions=['text'], self_harm_intent=['text'], sexual=['text'], sexual_minors=['text'], violence=['text'], violence_graphic=['text'], harassment/threatening=['text'], hate/threatening=['text'], illicit/violent=['text'], self-harm/intent=['text'], self-harm/instructions=['text'], self-harm=['text'], sexual/minors=['text'], violence/graphic=['text']), category_scores=CategoryScores(harassment=3.353501304664781e-05, harassment_threatening=1.6603846953733614e-05, hate=7.484622751061123e-06, hate_threatening=9.223470110117277e-07, illicit=3.250358452365473e-05, illicit_violent=2.453694805426225e-05, self_harm=9.314593042575101e-06, self_harm_instructions=3.4268490542555014e-06, self_harm_intent=0.0002117642425769555, sexual=5.193048644949271e-05, sexual_minors=6.814872211615988e-06, violence=0.0005633490590837672, violence_graphic=4.832563818725537e-06, harassment/threatening=1.6603846953733614e-05, hate/threatening=9.223470110117277e-07, illicit/violent=2.453694805426225e-05, self-harm/intent=0.0002117642425769555, self-harm/instructions=3.4268490542555014e-06, self-harm=9.314593042575101e-06, sexual/minors=6.814872211615988e-06, violence/graphic=4.832563818725537e-06), flagged=False)])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Image\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generate_image(prompt):\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",   # 최신 이미지 모델 (DALL·E 3 기반)\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"hd\",   # ← 여기 수정!\n",
        "        n=1\n",
        "    )\n",
        "    image_url = response.data[0].url\n",
        "    return image_url\n",
        "\n",
        "def save_image(image_url, filename):\n",
        "    response = requests.get(image_url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    image.save(filename)\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"A white siamese cat\"\n",
        "\n",
        "# Generate image\n",
        "image_url = generate_image(prompt)\n",
        "print(f\"Image URL: {image_url}\")\n",
        "\n",
        "# Save image to file\n",
        "save_image(image_url, \"generated_image.png\")\n",
        "print(\"Image saved as generated_image.png\")"
      ],
      "metadata": {
        "id": "z4UtdvCztyf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9c015a-b08a-46dc-d596-dbe2177bcf62"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-EapDaOUXOl9MtYwIDoZcAdKo/user-C2hyOA2hT2IjWzanW1PUVYp9/img-HnvrnCregS7OByuAdstkzErG.png?st=2025-11-26T04%3A32%3A58Z&se=2025-11-26T06%3A32%3A58Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=32836cae-d25f-4fe9-827b-1c8c59c442cc&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-11-26T05%3A32%3A58Z&ske=2025-11-27T05%3A32%3A58Z&sks=b&skv=2024-08-04&sig=idZyyai/mg9tMcVw9zUKo794pZUovBNsoxgmfCYTKqo%3D\n",
            "Image saved as generated_image.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Codex\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generate_code(prompt, model=\"gpt-4.1-mini\", max_tokens=800):\n",
        "    try:\n",
        "        response = client.responses.create(\n",
        "            model=model,\n",
        "            input=prompt,\n",
        "            max_output_tokens=max_tokens,\n",
        "            temperature=0\n",
        "        )\n",
        "        # Extract the generated code\n",
        "        code = response.output_text.strip()\n",
        "        return code\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Write a C code that computes Fibonacci number using memoization.\"\n",
        "\n",
        "# Generate code\n",
        "generated_code = generate_code(prompt)\n",
        "\n",
        "# Print result\n",
        "if generated_code:\n",
        "    print(\"Generated Code:\\n\")\n",
        "    print(generated_code)\n",
        "else:\n",
        "    print(\"Failed to generate code.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY9VB-b7k4uh",
        "outputId": "44168a20-5614-4eca-e570-09d8c7171bf4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "\n",
            "Certainly! Below is a C program that computes Fibonacci numbers using memoization. It uses an array to store previously computed Fibonacci values to avoid redundant calculations.\n",
            "\n",
            "```c\n",
            "#include <stdio.h>\n",
            "#include <stdlib.h>\n",
            "\n",
            "// Function to compute Fibonacci number using memoization\n",
            "long long fibonacci(int n, long long memo[]) {\n",
            "    if (n <= 1) {\n",
            "        return n;\n",
            "    }\n",
            "    if (memo[n] != -1) {\n",
            "        return memo[n];\n",
            "    }\n",
            "    memo[n] = fibonacci(n - 1, memo) + fibonacci(n - 2, memo);\n",
            "    return memo[n];\n",
            "}\n",
            "\n",
            "int main() {\n",
            "    int n;\n",
            "    printf(\"Enter the Fibonacci term to compute: \");\n",
            "    scanf(\"%d\", &n);\n",
            "\n",
            "    if (n < 0) {\n",
            "        printf(\"Please enter a non-negative integer.\\n\");\n",
            "        return 1;\n",
            "    }\n",
            "\n",
            "    // Allocate memory for memoization array and initialize with -1\n",
            "    long long *memo = (long long *)malloc((n + 1) * sizeof(long long));\n",
            "    if (memo == NULL) {\n",
            "        printf(\"Memory allocation failed.\\n\");\n",
            "        return 1;\n",
            "    }\n",
            "\n",
            "    for (int i = 0; i <= n; i++) {\n",
            "        memo[i] = -1;\n",
            "    }\n",
            "\n",
            "    long long result = fibonacci(n, memo);\n",
            "    printf(\"Fibonacci number F(%d) = %lld\\n\", n, result);\n",
            "\n",
            "    free(memo);\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "- The `fibonacci` function checks if the value has already been computed (memoized). If yes, it returns the stored value.\n",
            "- If not, it computes the value recursively and stores it in the `memo` array.\n",
            "- The `memo` array is initialized with `-1` to indicate uncomputed values.\n",
            "- The program reads an integer `n` from the user and computes the `n`th Fibonacci number.\n",
            "\n",
            "You can compile and run this program using:\n",
            "```bash\n",
            "gcc -o fib_memo fib_memo.c\n",
            "./fib_memo\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FIlE\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Upload a file\n",
        "upload_response = client.files.create(\n",
        "    file=open(\"king-style-chat.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(\"Upload Response:\")\n",
        "print(upload_response)\n",
        "\n",
        "# List all files\n",
        "list_response = client.files.list()\n",
        "print(\"List Response:\")\n",
        "print(list_response)\n",
        "\n",
        "# Retrieve a specific file\n",
        "file_id = upload_response.id\n",
        "retrieve_response = client.files.retrieve(file_id)\n",
        "print(\"Retrieve Response:\")\n",
        "print(retrieve_response)\n",
        "\n",
        "# Delete a file\n",
        "delete_response = client.files.delete(file_id)\n",
        "print(\"Delete Response:\")\n",
        "print(delete_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGFtcb9UlH5R",
        "outputId": "1a74044d-ac8f-4163-f9dd-9b1ca5461970"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload Response:\n",
            "FileObject(id='file-BwKVSosjhzx8vWnheDRukG', bytes=57615, created_at=1764135191, filename='king-style-chat.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
            "List Response:\n",
            "SyncCursorPage[FileObject](data=[FileObject(id='file-BwKVSosjhzx8vWnheDRukG', bytes=57615, created_at=1764135191, filename='king-style-chat.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-YTpmFHnPiN6qiXeYWjkjcH', bytes=2237, created_at=1764134239, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-5AABes5syEFz5WfFjzqa4e', bytes=2237, created_at=1764050684, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-YNtye4UpTqrXddbomFnJbQ', bytes=2237, created_at=1764049974, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-DPFnWUwKV9Wbyo7dfNiCoo', bytes=2237, created_at=1764049930, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-85WjEmhNh23EKYCH2QEXXC', bytes=2237, created_at=1764049496, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-9yyy13qLg6VoxjVwsTrh3y', bytes=2237, created_at=1764049394, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-MCawtVPfjsBXoPnuiASYMC', bytes=2237, created_at=1764049370, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-RmGLLNTEXB6CiC5rzRYhDg', bytes=2237, created_at=1764049314, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-HU1tEkHCS8sRNC6VpMNM6j', bytes=2237, created_at=1764048973, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-Xa2Buq2RBpkUXV4deSLNyH', bytes=2237, created_at=1764048906, filename='mydata2.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)], has_more=False, object='list', first_id='file-BwKVSosjhzx8vWnheDRukG', last_id='file-Xa2Buq2RBpkUXV4deSLNyH')\n",
            "Retrieve Response:\n",
            "FileObject(id='file-BwKVSosjhzx8vWnheDRukG', bytes=57615, created_at=1764135191, filename='king-style-chat.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
            "Delete Response:\n",
            "FileDeleted(id='file-BwKVSosjhzx8vWnheDRukG', deleted=True, object='file')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Audio\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def transcribe_audio(file_path, model=\"whisper-1\", temperature=0.1, prompt=None):\n",
        "    with open(file_path, \"rb\") as audio_file:\n",
        "        response = client.audio.transcriptions.create(\n",
        "            model=model,\n",
        "            file=audio_file,\n",
        "            temperature=temperature,\n",
        "            prompt=prompt,\n",
        "        )\n",
        "    return response.text\n",
        "\n",
        "file_path = \"Dracula.mp3\"\n",
        "transcription = transcribe_audio(file_path)\n",
        "\n",
        "print(\"Transcription Response:\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJzs4lxFlqGY",
        "outputId": "b07c5ffe-9896-4e08-8c0d-a1cfffa793ea"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription Response:\n",
            "Now that we've found where the enemy's lurking, nothing can stand in our way. Since we are facing the forces of darkness, we must be the cold light of day. We are the lanterns that burn in the lighthouse. The candles in the crypt. We are the light. Let there be light. This is a war and we must be the victors. There's too much to lose if we fail. We'll cross the seas like a band of crusaders, searching for some precious grail. We are the embers that glow in the winter, the diamonds in the mine. Let's take our torches and pray God will show us a sign. Deep in the darkness night, when there's the spark of hope, we must be voice of light. He's in the darkness, bright as the dazzling stars in a different sky. And in our cruelest hour, when hope is gone, we'll raise our heads and we'll turn the odds. When the great battle commences, surely the light will prevail. We will break down his defenses, he will fall. And the sun will rise. Deep in the darkness night, when there's the spark of hope, we must be voice of light. He's in the darkness, bright as the dazzling stars in a different sky. And in our cruelest hour, when hope is gone, we'll raise our heads and we'll turn the odds.\n"
          ]
        }
      ]
    }
  ]
}